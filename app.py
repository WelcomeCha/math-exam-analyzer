import streamlit as st
import google.generativeai as genai
from google.generativeai import caching
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import os
import tempfile
import time
import markdown
import pypdf
import datetime

# 1. ì„¤ì •
st.set_page_config(page_title="ìˆ˜í•™ ê¸°ì¶œ ë¶„ì„ê¸° (2.5 Pro Caching)", layout="wide")
st.markdown("""
    <style>
    div[data-testid="stMarkdownContainer"] p, td, th { font-family: 'Malgun Gothic', sans-serif !important; }
    .success-log { color: #2e7d32; font-weight: bold; }
    .info-log { color: #0277bd; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ’¯ ìˆ˜í•™ ê¸°ì¶œ ë¶„ì„ê¸° (2.5 Pro ê³ ì • + ìºì‹±)")

# 2. ì„¸ì…˜
if 'analysis_history' not in st.session_state:
    st.session_state['analysis_history'] = []
if 'last_index' not in st.session_state:
    st.session_state['last_index'] = 0
if 'cache_name' not in st.session_state:
    st.session_state['cache_name'] = None

# 3. API í‚¤
with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("Google API Key", type="password")
    st.divider()
    st.info("ğŸ”’ **ëª¨ë¸ ê³ ì •:** Gemini 2.5 Pro")
    st.info("ğŸ’¾ **ê¸°ëŠ¥:** 2.5 Pro ëª¨ë¸ì— ìºì‹±ì„ ì ìš©í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.")
    
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
        genai.configure(api_key=api_key)

# 4. íŒŒì¼ ì—…ë¡œë“œ
col1, col2 = st.columns(2)
with col1:
    exam_file = st.file_uploader("ê¸°ì¶œ PDF", type=['pdf'])
with col2:
    textbook_files = st.file_uploader("ë¶€êµì¬ PDF (ë‹¤ì¤‘)", type=['pdf'], accept_multiple_files=True)

# í•¨ìˆ˜ë“¤
def upload_to_gemini(file_obj):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(file_obj.getvalue())
        tmp_path = tmp.name
    return genai.upload_file(tmp_path, mime_type="application/pdf")

def wait_for_files(files):
    with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
        for f in files:
            while genai.get_file(f.name).state.name == "PROCESSING":
                time.sleep(1)

def create_html(text_list):
    full_text = "\n\n".join(text_list)
    html_body = markdown.markdown(full_text, extensions=['tables'])
    return f"""
    <html><head><meta charset="utf-8">
    <script>MathJax={{tex:{{inlineMath:[['$','$']],displayMath:[['$$','$$']]}},svg:{{fontCache:'global'}} }};</script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>body{{font-family:'Malgun Gothic';padding:40px;line-height:1.6}} table{{border-collapse:collapse;width:100%;margin-bottom:30px}} th,td{{border:1px solid #ddd;padding:15px}} th{{background:#007bff;color:white;text-align:center}}</style>
    </head><body>{html_body}</body></html>
    """

# 5. ì‹¤í–‰ ë¡œì§
if exam_file and textbook_files and api_key:
    batches = []
    for i in range(1, 26): batches.append((f"{i}ë²ˆ", f"ê¸°ì¶œ ê°ê´€ì‹ {i}ë²ˆ"))
    for i in range(1, 7): batches.append((f"ì„œë‹µí˜• {i}ë²ˆ", f"ê¸°ì¶œ ì„œë‹µí˜•(ì£¼ê´€ì‹) {i}ë²ˆ"))

    c1, c2 = st.columns(2)
    start_btn = c1.button("ğŸš€ ìºì‹± & ë¶„ì„ ì‹œì‘")
    resume_btn = False
    if st.session_state['last_index'] > 0:
        resume_btn = c2.button(f"â¯ï¸ {batches[st.session_state['last_index']][0]}ë¶€í„° ì´ì–´í•˜ê¸°")

    if start_btn or resume_btn:
        start_idx = 0 if start_btn else st.session_state['last_index']
        if start_btn: st.session_state['analysis_history'] = []

        try:
            # 1. íŒŒì¼ ì—…ë¡œë“œ
            status = st.empty()
            
            # ìºì‹œê°€ ì—†ê±°ë‚˜ ì²˜ìŒ ì‹œì‘ì´ë©´ ìƒˆë¡œ ìƒì„±
            if not st.session_state.get('cache_name') or start_btn:
                status.info("ğŸ“‚ íŒŒì¼ ì„œë²„ ì—…ë¡œë“œ ì¤‘...")
                uploaded_exam = upload_to_gemini(exam_file)
                uploaded_textbooks = [upload_to_gemini(f) for f in textbook_files]
                all_files = [uploaded_exam] + uploaded_textbooks
                
                wait_for_files(all_files)
                
                status.info("ğŸ’¾ 2.5 Pro ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„± ì¤‘...")
                
                # --- ğŸ”¥ [ìˆ˜ì • ì™„ë£Œ] ëª¨ë¸ëª…ì„ 2.5 Proë¡œ í™•ì‹¤í•˜ê²Œ ê³ ì • ---
                cache = caching.CachedContent.create(
                    model='models/gemini-2.5-pro', # 1.5 Pro ì‚­ì œ -> 2.5 Pro ì ìš©
                    display_name='math_exam_analysis_v2',
                    system_instruction="""
                    ë‹¹ì‹ ì€ ìˆ˜í•™ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
                    [ì›ì¹™]
                    1. ì ˆëŒ“ê°’ì€ ë°˜ë“œì‹œ `\\lvert x \\rvert` ì‚¬ìš©.
                    2. ë¶€êµì¬ ìœ ì‚¬ ë¬¸í•­ ë°˜ë“œì‹œ ë§¤ì¹­.
                    3. ì—†ëŠ” ê²½ìš°ì—ë§Œ "SKIP".
                    """,
                    contents=all_files,
                    ttl=datetime.timedelta(minutes=60)
                )
                st.session_state['cache_name'] = cache.name
                status.markdown(f"<p class='success-log'>âœ… ìºì‹œ ìƒì„± ì™„ë£Œ! (ID: {cache.name})</p>", unsafe_allow_html=True)
            else:
                # ì´ë¯¸ ìºì‹œê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (ì´ë¦„ìœ¼ë¡œ ê°€ì ¸ì˜´)
                cache = caching.CachedContent.get(st.session_state['cache_name'])
                status.info(f"â™»ï¸ ê¸°ì¡´ ìºì‹œ ì¬ì‚¬ìš© ì¤‘: {cache.name}")

            # 2. ëª¨ë¸ ì—°ê²° (2.5 Pro ìºì‹œ ì‚¬ìš©)
            model = genai.GenerativeModel.from_cached_content(cached_content=cache)
            
            # 3. ë¶„ì„ ë£¨í”„
            p_bar = st.progress(start_idx / len(batches))
            
            for i in range(start_idx, len(batches)):
                title, desc = batches[i]
                status.info(f"ğŸ”„ {title} ë¶„ì„ ì¤‘... (2.5 Pro)")
                
                prompt_text = f"""
                **{desc}**ì„ ë¶„ì„í•˜ì„¸ìš”.
                
                | ë¬¸í•­ | ê¸°ì¶œ ìš”ì•½ | ë¶€êµì¬ ìœ ì‚¬ ë¬¸í•­ | ìƒì„¸ ë³€í˜• ë¶„ì„ |
                | :--- | :--- | :--- | :--- |
                | {title} | **[ì›ë³¸]**<br>(LaTeX)<br><br>**[ìš”ì•½]** | **[ì›ë³¸]**<br>p.xx<br>(LaTeX)<br><br>**[ìš”ì•½]** | **â–¶ ë³€í˜• í¬ì¸íŠ¸**<br>â€¢ ë‚´ìš© |
                """
                
                success = False
                for attempt in range(3):
                    try:
                        current_prompt = prompt_text
                        if attempt == 1: current_prompt += "\n(ì €ì‘ê¶Œ í•„í„° íšŒí”¼: ë¬¸ì œ ì›ë¬¸ì€ í•µì‹¬ ìˆ˜ì¹˜ë§Œ ìš”ì•½í•˜ì„¸ìš”.)"
                        if attempt == 2: current_prompt += "\n(ê¸¸ì´ ì œí•œ íšŒí”¼: ë‚´ìš©ì„ ì•„ì£¼ ê°„ê²°í•˜ê²Œ ì¤„ì´ì„¸ìš”.)"
                        
                        resp = model.generate_content(current_prompt)
                        
                        if resp.parts:
                            txt = resp.text
                            st.session_state['analysis_history'].append(txt)
                            st.markdown(txt, unsafe_allow_html=True)
                            success = True
                            break
                    except Exception:
                        time.sleep(1)
                
                st.session_state['last_index'] = i + 1
                p_bar.progress((i + 1) / len(batches))
            
            status.success("âœ… ë¶„ì„ ì™„ë£Œ!")
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

    if st.session_state['analysis_history']:
        st.divider()
        html = create_html(st.session_state['analysis_history'])
        st.download_button("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", html, "ë¶„ì„ê²°ê³¼.html")
