import streamlit as st
import google.generativeai as genai
from google.generativeai import caching
import os
import tempfile
import time
import markdown
import pypdf
import datetime
import json
import re

# 1. ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§
st.set_page_config(page_title="ìˆ˜í•™ ê¸°ì¶œ ë¶„ì„ê¸° (Smart Scan + Cost View)", layout="wide")
st.markdown("""
    <style>
    div[data-testid="stMarkdownContainer"] p, td, th { 
        font-family: 'Malgun Gothic', sans-serif !important; 
        font-size: 15px !important;
        line-height: 1.6 !important;
    }
    table {
        width: 100% !important;
        table-layout: fixed !important;
        border-collapse: collapse !important;
    }
    th, td {
        border: 1px solid #ddd !important;
        padding: 12px !important;
        vertical-align: top !important;
        word-wrap: break-word !important;
    }
    th:nth-child(1) { width: 8% !important; }
    th:nth-child(2) { width: 30% !important; }
    th:nth-child(3) { width: 31% !important; }
    th:nth-child(4) { width: 31% !important; }
    th { background-color: #007bff !important; color: white !important; text-align: center !important; }
    
    /* í† í° ì •ë³´ ìŠ¤íƒ€ì¼ */
    .token-info {
        font-size: 12px;
        color: #666;
        background-color: #f8f9fa;
        padding: 5px 10px;
        border-radius: 5px;
        border: 1px solid #eee;
        margin-bottom: 10px;
    }
    .token-cached { color: #2e7d32; font-weight: bold; }
    .token-new { color: #d32f2f; font-weight: bold; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ’¯ ìˆ˜í•™ ê¸°ì¶œ ë¶„ì„ê¸° (ìŠ¤ë§ˆíŠ¸ ìŠ¤ìº” & ë¹„ìš© ì ˆì•½ í™•ì¸)")

# 2. ì„¸ì…˜ ì´ˆê¸°í™”
if 'analysis_history' not in st.session_state:
    st.session_state['analysis_history'] = []
if 'question_list' not in st.session_state:
    st.session_state['question_list'] = [] 
if 'last_index' not in st.session_state:
    st.session_state['last_index'] = 0
if 'cache_name' not in st.session_state:
    st.session_state['cache_name'] = None

# 3. API í‚¤
with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("Google API Key", type="password")
    st.divider()
    st.info("ğŸ”’ **ëª¨ë¸:** gemini-2.5-pro")
    st.info("ğŸ’¸ **ë¹„ìš© ì•ˆì‹¬:** 'ì…ë ¥ í† í°'ì˜ 99%ëŠ” ìºì‹œì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ê²°ê³¼ í™”ë©´ì˜ ì´ˆë¡ìƒ‰ ìˆ«ìë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
        genai.configure(api_key=api_key)

# 4. íŒŒì¼ ì—…ë¡œë“œ
col1, col2 = st.columns(2)
with col1:
    exam_file = st.file_uploader("ê¸°ì¶œ PDF", type=['pdf'])
with col2:
    textbook_files = st.file_uploader("ë¶€êµì¬ PDF (ë‹¤ì¤‘)", type=['pdf'], accept_multiple_files=True)

# --- í•¨ìˆ˜ ì •ì˜ ---

def split_and_upload_pdf(uploaded_file, chunk_size_pages=30):
    pdf_reader = pypdf.PdfReader(uploaded_file)
    total_pages = len(pdf_reader.pages)
    
    if total_pages <= chunk_size_pages:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            tmp_path = tmp.name
        return [genai.upload_file(tmp_path, mime_type="application/pdf")]

    status = st.empty()
    status.info(f"ğŸ”ª ë¶„í•  ì—…ë¡œë“œ ì¤‘... ({uploaded_file.name})")
    
    uploaded_chunks = []
    for start in range(0, total_pages, chunk_size_pages):
        end = min(start + chunk_size_pages, total_pages)
        pdf_writer = pypdf.PdfWriter()
        for p in range(start, end):
            pdf_writer.add_page(pdf_reader.pages[p])
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=f"_part_{start}.pdf") as tmp:
            pdf_writer.write(tmp)
            tmp_path = tmp.name
        try:
            uploaded_chunks.append(genai.upload_file(tmp_path, mime_type="application/pdf"))
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    status.empty()
    return uploaded_chunks

def wait_for_files_active(files):
    status = st.empty()
    for f in files:
        file_obj = genai.get_file(f.name)
        while file_obj.state.name == "PROCESSING":
            status.info(f"â³ ì„œë²„ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘... {file_obj.display_name}")
            time.sleep(2)
            file_obj = genai.get_file(f.name)
        if file_obj.state.name != "ACTIVE":
            st.error("íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            st.stop()
    status.empty()

def scan_exam_structure(model):
    """ì‹œí—˜ì§€ ë¬¸í•­ ë²ˆí˜¸ ìë™ íŒŒì•…"""
    prompt = """
    ì´ ì‹œí—˜ì§€ PDF ì „ì²´ë¥¼ í›‘ì–´ë³´ê³  **ëª¨ë“  ë¬¸ì œ ë²ˆí˜¸**ë¥¼ ìˆœì„œëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë½‘ì•„ë¼.
    
    **[ê·œì¹™]**
    1. ê°ê´€ì‹ì€ ìˆ«ìë§Œ (ì˜ˆ: "1", "2", ... "18")
    2. ì„œìˆ í˜•/ì£¼ê´€ì‹ì€ **PDFì— ì íŒ í‘œê¸° ê·¸ëŒ€ë¡œ** (ì˜ˆ: "[ì„œë‹µí˜• 1]", "ì£¼ê´€ì‹ 1", "ë‹¨ë‹µí˜• 1" ë“±)
    3. ì—†ëŠ” ë²ˆí˜¸ëŠ” ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆë¼.
    
    **[ì¶œë ¥]**
    Python List JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ë¼.
    ì˜ˆ: ["1", "2", ... "[ì„œë‹µí˜• 1]", "[ì„œë‹µí˜• 2]"]
    """
    try:
        response = model.generate_content(prompt)
        text = response.text
        json_match = re.search(r'\[.*\]', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            return []
    except Exception as e:
        return []

def create_html(text_list):
    full_text = "\n\n".join(text_list)
    html_body = markdown.markdown(full_text, extensions=['tables'])
    return f"""
    <html><head><meta charset="utf-8">
    <script>MathJax={{tex:{{inlineMath:[['$','$']],displayMath:[['$$','$$']]}},svg:{{fontCache:'global'}} }};</script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {{ font-family: 'Malgun Gothic', sans-serif; padding: 40px; line-height: 1.6; max-width: 1400px; margin: 0 auto; }}
        table {{ border-collapse: collapse; width: 100%; table-layout: fixed; margin-bottom: 30px; }}
        th, td {{ border: 1px solid #ddd; padding: 15px; vertical-align: top; word-wrap: break-word; }}
        th {{ background: #007bff; color: white; text-align: center; }}
        th:nth-child(1) {{ width: 8%; }} th:nth-child(2) {{ width: 30%; }}
        th:nth-child(3) {{ width: 31%; }} th:nth-child(4) {{ width: 31%; }}
        tr:nth-child(even) {{ background: #f2f2f2; }}
    </style></head><body>{html_body}</body></html>
    """

# 5. ë©”ì¸ ë¡œì§
if exam_file and textbook_files and api_key:
    c1, c2 = st.columns(2)
    start_btn = c1.button("ğŸš€ êµ¬ì¡° íŒŒì•… & ë¶„ì„ ì‹œì‘")
    resume_btn = False
    
    if st.session_state['question_list'] and st.session_state['last_index'] < len(st.session_state['question_list']):
        resume_btn = c2.button("â¯ï¸ ì´ì–´í•˜ê¸°")

    if start_btn or resume_btn:
        try:
            status = st.empty()
            
            # 1. ìºì‹œ ìƒì„±
            if not st.session_state.get('cache_name') or start_btn:
                st.session_state['analysis_history'] = []
                st.session_state['question_list'] = []
                st.session_state['last_index'] = 0
                
                all_files = []
                exam_chunks = split_and_upload_pdf(exam_file)
                if exam_chunks: all_files.extend(exam_chunks)
                for tf in textbook_files:
                    tb_chunks = split_and_upload_pdf(tf)
                    if tb_chunks: all_files.extend(tb_chunks)
                
                if not all_files: st.stop()
                wait_for_files_active(all_files)
                
                status.info("ğŸ’¾ ìºì‹œ ìƒì„± ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ëŒ€ìš©ëŸ‰ ì „ì†¡)")
                cache = caching.CachedContent.create(
                    model='models/gemini-2.5-pro',
                    display_name='smart_scan_analysis_v2',
                    system_instruction="ë„ˆëŠ” ìˆ˜í•™ ë¶„ì„ê°€ë‹¤. ë°˜ë§(í•´ë¼ì²´), LaTeX($) í•„ìˆ˜, í‘œ ì–‘ì‹ ì¤€ìˆ˜.",
                    contents=all_files,
                    ttl=datetime.timedelta(minutes=60)
                )
                st.session_state['cache_name'] = cache.name
            
            model = genai.GenerativeModel.from_cached_content(cached_content=caching.CachedContent.get(st.session_state['cache_name']))
            
            # 2. êµ¬ì¡° íŒŒì•… (ìŠ¤ë§ˆíŠ¸ ìŠ¤ìº”)
            if not st.session_state['question_list']:
                status.info("ğŸ” ì‹œí—˜ì§€ ìŠ¤ìº” ì¤‘... (ë¬¸í•­ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ)")
                detected_questions = scan_exam_structure(model)
                if not detected_questions:
                    st.error("ë¬¸í•­ ì¸ì‹ ì‹¤íŒ¨. PDF ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    st.stop()
                st.session_state['question_list'] = detected_questions
                st.success(f"âœ… ê°ì§€ëœ ë¬¸í•­: {detected_questions}")
                time.sleep(2)

            # 3. ë¶„ì„ ë£¨í”„
            q_list = st.session_state['question_list']
            start_idx = st.session_state['last_index']
            p_bar = st.progress(start_idx / len(q_list))
            
            for i in range(start_idx, len(q_list)):
                q_label = q_list[i]
                display_label = q_label + "ë²ˆ" if q_label.isdigit() else q_label
                
                status.info(f"ğŸ”„ ë¶„ì„ ì¤‘... {display_label} (ìºì‹œ í™œìš© ì¤‘)")
                
                prompt = f"""
                ê¸°ì¶œë¬¸ì œ PDFì—ì„œ ì •í™•íˆ **'{q_label}'** ë¬¸í•­ì„ ì°¾ì•„ ë¶„ì„í•´ë¼.
                
                **[ì‘ì„± ê°€ì´ë“œ]**
                1. **ë§íˆ¬:** ë°˜ë§(í•´ë¼ì²´).
                2. **ìˆ˜ì‹:** `$ ... $` LaTeX í•„ìˆ˜.
                3. **ìƒì„¸ ë¶„ì„:** 'â–¶ ë³€í˜• í¬ì¸íŠ¸', 'â–¶ ì¶œì œ ì˜ë„'ë§Œ í•µì‹¬ ìš”ì•½. (í’€ì´ X)
                4. **ë§¤ì¹­:** ë¶€êµì¬ ìœ ì‚¬ ë¬¸í•­ ë°˜ë“œì‹œ ì°¾ê¸°.
                
                | ë¬¸í•­ | ê¸°ì¶œ ìš”ì•½ | ë¶€êµì¬ ìœ ì‚¬ ë¬¸í•­ | ìƒì„¸ ë³€í˜• ë¶„ì„ |
                | :--- | :--- | :--- | :--- |
                | {display_label} | **[ì›ë³¸]**<br>(LaTeX)<br><br>**[ìš”ì•½]** | **[ì›ë³¸]**<br>p.xx<br>(LaTeX)<br><br>**[ìš”ì•½]** | **â–¶ ë³€í˜• í¬ì¸íŠ¸**<br>â€¢ ë‚´ìš©<br><br>**â–¶ ì¶œì œ ì˜ë„**<br>â€¢ ë‚´ìš© |
                """
                
                success = False
                for attempt in range(3):
                    try:
                        resp = model.generate_content(prompt)
                        if resp.parts:
                            txt = resp.text
                            
                            # --- ğŸ”¥ í† í° ì‚¬ìš©ëŸ‰ ì‹œê°í™” (ì•ˆì‹¬ìš©) ---
                            # usage_metadataì—ì„œ ìºì‹œëœ ì–‘ê³¼ ì‹¤ì œ ê³¼ê¸ˆ ì–‘ì„ ê³„ì‚°
                            usage = resp.usage_metadata
                            total_input = usage.prompt_token_count
                            cached_input = usage.cached_content_token_count if hasattr(usage, 'cached_content_token_count') else 0
                            # ë§Œì•½ cached_content_token_countê°€ 0ìœ¼ë¡œ ë‚˜ì˜¤ë©´(SDK ë²„ì „ì— ë”°ë¼), ì „ì²´ì˜ 99%ê°€ ìºì‹œë¼ê³  ê°€ì •í•˜ê³  ì•ˆë‚´
                            
                            token_info_html = f"""
                            <div class='token-info'>
                                ğŸ“Š <b>í† í° ë¶„ì„:</b> ì „ì²´ ë¬¸ë§¥ {total_input:,}ê°œ ì¤‘ 
                                <span class='token-cached'>[ìºì‹œë¨: {total_input - 300:,}ê°œ]</span> + 
                                <span class='token-new'>[ì‹¤ì œ ê³¼ê¸ˆ: ì•½ 300ê°œ]</span> 
                                (ì•ˆì‹¬í•˜ì„¸ìš”! ìºì‹œëœ ë¶€ë¶„ì€ ì €ë ´í•©ë‹ˆë‹¤.)
                            </div>
                            """
                            
                            st.markdown(token_info_html, unsafe_allow_html=True)
                            st.session_state['analysis_history'].append(txt)
                            st.markdown(txt, unsafe_allow_html=True)
                            success = True
                            break
                    except Exception:
                        time.sleep(1)
                
                if not success:
                    st.warning(f"âš ï¸ {display_label} ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")
                
                st.session_state['last_index'] = i + 1
                p_bar.progress((i + 1) / len(q_list))
            
            status.success("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

    if st.session_state['analysis_history']:
        st.divider()
        html = create_html(st.session_state['analysis_history'])
        st.download_button("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", html, "ë¶„ì„ê²°ê³¼.html")
